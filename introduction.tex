\chapter{Introduction} % (fold)
\label{cha:introduction}

In this chapter we present our motivation and objectives for this work, and we
present the organization of the document.

\section{Motivation}

Nowadays big companies are processing and daily generating a vast amount of data.
These companies are growing investing in distributed and parallel computing
to process such data. To perform the distributed computing efficiently the data
storage must be simple in order to allow parallel processing. The key-value model
is a potential solution to enable the applications enjoy the data parallelism,
e.g. the MapReduce (MR) \nomenclature{$MR$}{- MapReduce} programing paradigm which is based on key-value model~\cite{Dean:2004}.

MapReduce became the industry de facto standard for parallel processing.
Attractive features such as scalability and reliability motivate many large companies
such as Facebook, Google, Yahoo and research institutes to adopt this new programming
paradigm. Key-value model and MR paradigm are implemented on the framework Hadoop,
an open-source implementation of MapReduce, and these organizations rely
on Hadoop~\cite{White:2009} to process their information. Besides Hadoop, several
other implementations are available: Greenplum MapReduce~\cite{PivotalGreenplum:2008, Greenplum:2008},
Aster Data~\cite{Aster:2011}, Nokia Disco~\cite{Mundkur:2011} and
Microsoft Dryad~\cite{Isard:2007}.

MapReduce has a simplified programming model, where data processing algorithms 
are implemented as instances of two higher-order functions: Map and Reduce. All 
complex issues related to distributed processing, such as scalability, data
distribution and reconciliation, concurrence and fault tolerance are managed
by the framework. The main complexity that is left to the developer of a 
MapReduce-based application (also called a job) lies in the design decisions made 
to split the application specific algorithm into the two higher-order functions.
Even if some decisions may result in a functionally correct application, bad design
choices might also lead to poor performance.

Implement jobs on Hadoop is simple, but there are many knobs to adjust depending
on the available resources (e.g. input data, online machines, network bandwidth, etc.)
that improve the job performance. One relevant aspect is that the MR jobs are expected
to work with large amounts of data, which can be the main barrier to find a
configuration~\cite{Chen:2012} that adapts to the current cluster state, such
configuration we call of adaptive configuration. Therefore, data sampling can be
useful to improve the testing time of new configuration parameters,
instead of processing all data set how is done in~\cite{starfish}. But generate
a representative and relevant data sampling is hard and a bad sampling may not
represent several aspects related to the computation in large-scale: efficient
resource usage, correct merge of data and intermediate data.

\section{Objectives}
Our objective is to propose a self-tuning based on data sampling over Hadoop, so
we intend to use an evolutionary algorithm \cite{baudry} to select adaptive configurations
for MR jobs. Based on our knowledge the best way to test the job with such configurations
is to run it and analyse the response time, normally this process is done manually.
But a crucial trouble is the large amount of data stored that can exponentially 
increase the time needed to test the job. One way to solve this trouble is to create
a data sample. We propose one method to implement data sampling distributed using
key-value model and MR paradigm.

\section{Contribution}

We present an original approach to automate Hadoop job configuration. Our
approach is based on an bacteriological algorithm \cite{baudry} in order not to
run it on all data storage we develop a distributed method to obtain data samples
in Big Data environment. For data sampling we considered a lot of aspects related
the paradigm MR, key-value model and others hadoop particularities. Our approach
consists of a user interface to facilitate the user iteraction with data sampling
and drive tuning procedures through a domain-specific language (DSL).

Our proposal intents to establish a framework to automate Hadoop job configuration,
through the following proposals:
\begin{itemize}
	\item an algorithm to automate a self-tuning;
	\item a method for sampling data on Hadoop clusters;
	\item an interface for users based on domain specific language;
\end{itemize}

As measure of performance, we used the job response time. Furthermore, we intend
to use other measures of performance such amount of intermidiate data,
network usage and CPU usage.

\section{Outline}

The Chapter \ref{cha:background} introduces the fundamental concepts of the
key-value model, MR paradigm and Hadoop framework. Chapter \ref{cha:bacAlg}
presents the bacteriological algorithm to choose job configurations. Chapter
\ref{cha:sample} presents the
distributed method to generate data sampling. Chapter \ref{cha:dsl} introduces
the concepts of the domain-specific language and our proposal DSL. Chapter \ref{cha:framework}
presents our initial implementation with all components. Chapter \ref{cha:experiments}
discuss a case study performed with our solution. Chapter \ref{cha:conclusion}
conclude our results.

%The next section introduces the  fundamental concepts of the MapReduce framework and briefly describes different testing scenarios for MapReduce applications. Chapter~\ref{cha:proposal} introduces two test quality assessment methods, describes methods to data test generation and meta-heuristic search methods. Two data test generators are presented with their results.
%Chapter~\ref{cha:related} discusses related work. 
%Chapter~\ref{cha:conclusion} concludes.

% chapter introduction (end)


