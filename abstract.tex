% abstract
Currently with the popularity of internet and phenomenon of the social
networks a large amount of data is generated day-to-day. MapReduce appears as a
powerful paradigm to analyse and process such amount of data. The Hadoop framework
implements the MapReduce paradigm, in which a simple interface is available to
implement MapReduce(MR) jobs. However, in MR jobs developers are allowed to setup several
parameters to draw optimal performance from the available resources, but find a
good configuration is time consuming and a configuration found in an execution
may be impracticable for the next time.

In order to facilitate and automate tuning hadoop jobs, we propose a self-tuning
based on data sampling. Our approach allows to find a good job configuration considering
the data stored and the job in question. The users till can provide their usual
job configurations then get the new job configuration that will be more appropriate
with the current state of data stored and the hadoop cluster. So the users have
end-to-end tool to automate the choice of knobs for each job.
